{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qU2yJ9BRqcCJ"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3lOfG_3o2fNY"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.308150</td>\n",
              "      <td>3.378029</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-16.243747</td>\n",
              "      <td>-16.011393</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.584490</td>\n",
              "      <td>2.642162</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-15.965623</td>\n",
              "      <td>-12.119112</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-18.601721</td>\n",
              "      <td>15.422406</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           a          b  label\n",
              "0   7.308150   3.378029      1\n",
              "1 -16.243747 -16.011393      0\n",
              "2   7.584490   2.642162      1\n",
              "3 -15.965623 -12.119112      0\n",
              "4 -18.601721  15.422406      1"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#importing data\n",
        "data = pd.read_csv(\"HW4.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XVadXAZs2gKi"
      },
      "outputs": [],
      "source": [
        "#Taking feature values and Classification values as x and y\n",
        "x,y = data.iloc[:,:-1].values,data.iloc[:,-1].values\n",
        "y = np.array(y).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7DzbW5zu2itj"
      },
      "outputs": [],
      "source": [
        "#Number of neurons per layer without the bias\n",
        "neurons_per_layer = [2,2,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y0_oFzCd2l8E"
      },
      "outputs": [],
      "source": [
        "#Initializing the parameters W and b for each layer\n",
        "def init_parameters(neurons_per_layer):\n",
        "  np.random.seed(2021)\n",
        "  num_layer = len(neurons_per_layer)\n",
        "  param = {}\n",
        "  for i in range(1, num_layer):\n",
        "    param[\"W\" + str(i)] = np.random.randn(neurons_per_layer[i], neurons_per_layer[i - 1]) * 0.01\n",
        "    param[\"b\" + str(i)] = np.ones((neurons_per_layer[i], 1))\n",
        "  return param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kw-cWNDr2oUq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'W1': array([[ 0.01488609,  0.00676011],\n",
              "        [-0.00418451, -0.00806521]]),\n",
              " 'b1': array([[1.],\n",
              "        [1.]]),\n",
              " 'W2': array([[ 0.00555876, -0.00705504]]),\n",
              " 'b2': array([[1.]])}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Parameters W and b Initialized\n",
        "parameters = init_parameters(neurons_per_layer)\n",
        "parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Er-zQ3rM2wh7"
      },
      "outputs": [],
      "source": [
        "#Activation Functions\n",
        "\n",
        "#Sigmoid\n",
        "def sigmoid_fn(z):\n",
        "  a = 1 / (1 + np.exp(-z))\n",
        "  return a, z\n",
        "\n",
        "#Tan_h\n",
        "def tanh_fn(z):\n",
        "  a = np.tanh(z)\n",
        "  return a, z\n",
        "  \n",
        "#Relu\n",
        "def relu_fn(z):\n",
        "  a = np.maximum(0, z)\n",
        "  return a, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v8clBv4L22hQ"
      },
      "outputs": [],
      "source": [
        "#Log loss Function\n",
        "def logloss(y,y_pred):\n",
        "  loss = -np.mean(np.dot(y,np.log(y_pred)) + np.dot(1-y,np.log(1-y_pred)))\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4mHfe8xE25kS"
      },
      "outputs": [],
      "source": [
        "#Forward Propogation\n",
        "#Activation Function Value\n",
        "def activation_fun(W,b,a_prev,active_fn):\n",
        "  if active_fn == \"sigmoid_fn\":\n",
        "    z, lin_mem = forward_prop(W,b,a_prev)\n",
        "    a, act_mem = sigmoid_fn(z)\n",
        "  elif active_fn == \"relu_fn\":\n",
        "    z, lin_mem = forward_prop(W,b,a_prev)\n",
        "    a, act_mem = relu_fn(z)\n",
        "  elif active_fn == \"tanh_fn\":\n",
        "    z, lin_mem = forward_prop(W,b,a_prev)\n",
        "    a, act_mem = tanh_fn(z)\n",
        "  memory = (lin_mem, act_mem)\n",
        "  return a, memory\n",
        "def forward_prop(W,b,a_prev):\n",
        "  z = np.dot(W,a_prev) + b #Calculating Z = WX + b\n",
        "  memory = (a_prev, W, b)  #Storing\n",
        "  return z, memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PtKQ_6JF3BM2"
      },
      "outputs": [],
      "source": [
        "#Returning final output and Stored Parameters\n",
        "def for_model(X, param, active_func=\"relu_fn\"):\n",
        "  a = X.T\n",
        "  memories = [] #To store parameters during Forward Prop\n",
        "  L = len(param) // 2\n",
        "  for i in range(1, L):\n",
        "    a_prev = a\n",
        "    a, memory = activation_fun(param[\"W\" + str(i)], param[\"b\" + str(i)],a_prev,active_fn=active_func)\n",
        "    memories.append(memory)\n",
        "  y_pred, memory = activation_fun(param[\"W\" + str(L)], param[\"b\" + str(L)],a,active_fn=\"sigmoid_fn\")\n",
        "  memories.append(memory)\n",
        "  return y_pred, memories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MrpofdUR3Dfb"
      },
      "outputs": [],
      "source": [
        "y_pred, memory = for_model(x, parameters, active_func=\"relu_fn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ORkdD3To3ED5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.81286482304987"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Cost of 1 Forward Propogation\n",
        "cost_forward = logloss(y,y_pred)\n",
        "cost_forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vXccux_73FjM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.73098845 0.73010735 0.73098088 ... 0.73103767 0.73101252 0.73100544]]\n"
          ]
        }
      ],
      "source": [
        "#Final Output\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aFaFOsuI3Hbf"
      },
      "outputs": [],
      "source": [
        "#Backward Propogation\n",
        "def helper2(dz, lin_mem,active_fn = \"sigmoid_fn\"):\n",
        "  a_prev, W, b = lin_mem\n",
        "  dW = np.dot(dz, a_prev.T)\n",
        "  db = np.sum(dz, axis=1, keepdims=True)\n",
        "  da_prev = np.dot(W.T,dz)\n",
        "  return da_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NvoZGmIu3JRw"
      },
      "outputs": [],
      "source": [
        "#Differentiation of Activation Functions\n",
        "def sigmoid_diff(da, z):\n",
        "  a, z = sigmoid_fn(z)\n",
        "  dz = da * a * (1 - a)\n",
        "  return dz\n",
        "def tanh_diff(da, z):\n",
        "  a, z = tanh_fn(z)\n",
        "  dz = da * (1 - np.square(a))\n",
        "  return dz\n",
        "def relu_diff(da, z):\n",
        "  dz = np.where(da <= 0, 0, 1)\n",
        "  return dz\n",
        "def helper1(da, memory, active_fn):\n",
        "  lin_mem, act_mem = memory\n",
        "  if active_fn == \"relu_fn\":\n",
        "    dz = relu_diff(da, act_mem)\n",
        "    da_prev, dW, db = helper2(dz, lin_mem,active_fn)\n",
        "  elif active_fn == \"sigmoid_fn\":\n",
        "    dz = sigmoid_diff(da, act_mem)\n",
        "    da_prev, dW, db = helper2(dz, lin_mem,active_fn)\n",
        "  elif active_fn == \"tanh_fn\":\n",
        "    dz = tanh_diff(da, act_mem)\n",
        "    da_prev, dW, db = helper2(dz, lin_mem,active_fn)\n",
        "  return da_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "d1OD7EQh3L_r"
      },
      "outputs": [],
      "source": [
        "#Main for Back Propogation\n",
        "def back_prop(y,y_pred,memory,active_func=\"relu_fn\"):\n",
        "  gradient = {} #To store gradients after Back prop\n",
        "  y_new = y.T\n",
        "  lenght = len(memory)\n",
        "  #dloss\n",
        "  dy_pred = -1 * np.divide((y_new),(y_pred)) + np.divide(1-(y_new),1-(y_pred))\n",
        "  #Calculating Gradient for Output layer\n",
        "  gradient[\"da\" + str(lenght-1)], gradient[\"dW\" + str(lenght)], gradient[\"db\" + str(lenght)] = helper1(dy_pred, memory[lenght - 1],active_fn=\"sigmoid_fn\")\n",
        "  #Calculating Gradient for hidden layer\n",
        "  for i in range(lenght-1,0,-1):\n",
        "    gradient[\"da\" + str(i - 1)], gradient[\"dW\" + str(i)], gradient[\"db\" + str(i)] = helper1(gradient[\"da\" + str(i)], memory[i - 1],active_fn=\"relu_fn\")\n",
        "  gradient[\"da\" + str(lenght-1)] = dy_pred\n",
        "  return gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mAB33kiD3ctt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'da1': array([[-1.36801067,  3.70517688, -1.36802484, ..., -1.36791856,\n",
              "          3.71764517, -1.36797888]]),\n",
              " 'dW2': array([[ 924.97007311, 1232.0893135 ]]),\n",
              " 'db2': array([[1153.60055739]]),\n",
              " 'da0': array([[-0.00418451,  0.01488609, -0.00418451, ..., -0.00418451,\n",
              "          0.01488609, -0.00418451],\n",
              "        [-0.00806521,  0.00676011, -0.00806521, ..., -0.00806521,\n",
              "          0.00676011, -0.00806521]]),\n",
              " 'dW1': array([[-23484.54635964,   5373.18662014],\n",
              "        [-10533.46609323,  23204.88326723]]),\n",
              " 'db1': array([[2500],\n",
              "        [2500]])}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Gradients and Activation of all the layers after back propogation\n",
        "result = back_prop(y,y_pred,memory,active_func=\"relu_fn\")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "guGGuNgV3euW"
      },
      "outputs": [],
      "source": [
        "#Splitting data into Train,Test and Validation (0.6,0.2,0.2)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_rem, Y_train, Y_rem = train_test_split(x,y, train_size=0.6)\n",
        "X_valid, X_test, Y_valid, Y_test = train_test_split(X_rem,Y_rem, test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8xCzGC973iye"
      },
      "outputs": [],
      "source": [
        "#Running epoch with mini batch\n",
        "def final_call(neurons_per_layer,lamb):\n",
        "  epoch = 3000\n",
        "  L = len(neurons_per_layer)\n",
        "  alpha = 0.01 #Learning rate\n",
        "  number = x.shape[0]\n",
        "  batch_size = 500\n",
        "  num_batch = int(number / batch_size)\n",
        "  #initialization parameters\n",
        "  parameters = init_parameters(neurons_per_layer)\n",
        "  for i in range(1,epoch+1):\n",
        "    m=0\n",
        "    for z in range(num_batch):\n",
        "        #initialization of gradients for each batch\n",
        "        deltaw1 = 0\n",
        "        deltab1 = 0\n",
        "        deltaw2 = 0\n",
        "        deltab2 = 0\n",
        "        index = np.random.randint(low=0,high=5000,size=500)\n",
        "        x_mini = x[m:m+batch_size]\n",
        "        y_mini = y[m:m+batch_size]\n",
        "        m = m + batch_size\n",
        "\n",
        "        #Forward Prop\n",
        "        y_pred, memory = for_model(x_mini, parameters, active_func=\"relu_fn\")\n",
        "        #Backward Prop\n",
        "        backward = back_prop(y_mini,y_pred,memory,active_func=\"relu_fn\")\n",
        "        #Summing Gradients for each batch\n",
        "        deltaw1 = deltaw1 + backward[\"dW1\"]\n",
        "        deltab1 = deltab1 + backward[\"db1\"]\n",
        "        deltaw2 =  deltaw2 + backward[\"dW2\"]\n",
        "        deltab2 = deltab2 + backward[\"db2\"]\n",
        "      #Updating Parameters after each mini batch\n",
        "    parameters[\"W1\"] = parameters[\"W1\"] - alpha * ((1 / number) * deltaw1 + (lamb)*parameters[\"W1\"])\n",
        "    parameters[\"W2\"] = parameters[\"W2\"] - alpha * ((1 / number) * deltaw2 + (lamb)*parameters[\"W2\"])\n",
        "    parameters[\"b1\"] = parameters[\"b1\"] - alpha * (1 / number) * deltab1\n",
        "    parameters[\"b2\"] = parameters[\"b2\"] - alpha * (1 / number) * deltab2\n",
        "  return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IbUh0gdQ3vID"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'W1': array([[ 0.01488609,  0.00676011],\n",
            "       [-0.00418451, -0.00806521]]), 'b1': array([[1.],\n",
            "       [1.]]), 'W2': array([[ 0.00555876, -0.00705504]]), 'b2': array([[1.]])} \n",
            "\n",
            "[[0.73098845 0.73010735 0.73098088 ... 0.73103767 0.73101252 0.73100544]]\n"
          ]
        }
      ],
      "source": [
        "print(parameters,'\\n')\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gsMu8JZL3xPu"
      },
      "outputs": [],
      "source": [
        "#Neural Network with 3 Layers and S2 = 2\n",
        "neurons_per_layer = [2,2,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SNNpr4MH35cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss for lambda -> 0.01  is  0.7530753080867173\n",
            "Loss for lambda -> 10  is  0.7268174123015888\n",
            "Loss for lambda -> 0.5  is  0.7320513356799543\n",
            "Loss for lambda -> 7  is  0.7268350474333494\n",
            "Loss for lambda -> 20  is  0.7268034979571145\n",
            "Loss for lambda -> 5  is  0.7268208965394726\n"
          ]
        }
      ],
      "source": [
        "#Checking model for various lambdas\n",
        "lamb = [0.01,10,0.5,7,20,5]\n",
        "for l in lamb:\n",
        "  parameters = final_call(neurons_per_layer,l)\n",
        "  y_pred_valid, memory = for_model(X_valid, parameters, active_func=\"sigmoid_fn\")\n",
        "  cost_valid = logloss(Y_valid,y_pred_valid)\n",
        "  print(\"Loss for lambda ->\",l,\" is \",cost_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "54KEajdz3-EP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logloss for test set  0.7220152682503943\n"
          ]
        }
      ],
      "source": [
        "#Calculating Logloss for test split data\n",
        "param_test = final_call(neurons_per_layer,20)\n",
        "y_pred_test, memory = for_model(X_test, param_test, active_func=\"sigmoid_fn\")\n",
        "cost_test = logloss(Y_test,y_pred_test)\n",
        "print(\"Logloss for test set \", cost_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mcwxFZzo4y8V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logloss for training set  0.7196211533970358\n"
          ]
        }
      ],
      "source": [
        "#Calculating Logloss for test train data\n",
        "param_train = final_call(neurons_per_layer,20)\n",
        "y_pred_train, memory = for_model(X_train, param_train, active_func=\"sigmoid_fn\")\n",
        "cost_train = logloss(Y_train,y_pred_train)\n",
        "print(\"Logloss for training set \",cost_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mxUMzlfm400o"
      },
      "outputs": [],
      "source": [
        "#Neural Network with 3 Layers and S2 = 10\n",
        "neurons_per_layer = [2,10,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "l9mmrpGw4281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss for lambda -> 0.1  is  0.7541429897639004\n",
            "Loss for lambda -> 10  is  0.726922058008265\n",
            "Loss for lambda -> 0.5  is  0.7399815888490343\n",
            "Loss for lambda -> 7  is  0.7270058398765828\n",
            "Loss for lambda -> 20  is  0.726855873399464\n",
            "Loss for lambda -> 5  is  0.7269671534685541\n"
          ]
        }
      ],
      "source": [
        "#Checking model for various lambdas\n",
        "lamb = [0.1,10,0.5,7,20,5]\n",
        "for l in lamb:\n",
        "  param = final_call(neurons_per_layer,l)\n",
        "  y_pred_valid, memory = for_model(X_valid, param, active_func=\"sigmoid_fn\")\n",
        "  cost_valid = logloss(Y_valid,y_pred_valid)\n",
        "  print(\"Loss for lambda ->\",l,\" is \",cost_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2gs_WbWj45uN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logloss for test set  0.7220635683545326\n"
          ]
        }
      ],
      "source": [
        "#Calculating Logloss for test split data\n",
        "param_test = final_call(neurons_per_layer,20)\n",
        "y_pred_test, memory = for_model(X_test, param_test, active_func=\"sigmoid_fn\")\n",
        "cost_test = logloss(Y_test,y_pred_test)\n",
        "print(\"Logloss for test set \", cost_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "T-rAbgpA47hT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logloss for training set  0.719667415832066\n"
          ]
        }
      ],
      "source": [
        "#Calculating Logloss for test split data\n",
        "param_train = final_call(neurons_per_layer,20)\n",
        "y_pred_train, memory = for_model(X_train, param_train, active_func=\"sigmoid_fn\")\n",
        "cost_train = logloss(Y_train,y_pred_train)\n",
        "print(\"Logloss for training set \",cost_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "DoYBDEPV4_-M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x15b0e67f0>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Plotting Scatter plot for the two classes\n",
        "data1 = np.array(data)\n",
        "data1[:, 1]\n",
        "plt.plot(data1[:, 0][data1[:,-1]==0] , data1[:, 1][data1[:,-1]==0] , \"s\" , color = \"blue\", label=\"Class 1\")\n",
        "plt.plot(data1[:, 0][data1[:,-1]==1] , data1[:, 1][data1[:,-1]==1] , \"o\", color = \"green\", label=\"Class 2\")\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "SML HW4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
